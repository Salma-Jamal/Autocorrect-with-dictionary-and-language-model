{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AUTOCORRECT_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salmaag/NLP/blob/main/models/AUTOCORRECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vTEFaeL2B1k"
      },
      "source": [
        "!wget https://cdn.discordapp.com/attachments/778630432878362676/849552709488607242/data_qalb.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_ihrW_t2RO1"
      },
      "source": [
        "!unrar x /content/data_qalb.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YTMc22Tdt54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a22001d-776a-43ce-ae7a-862d0a441490"
      },
      "source": [
        "!wget https://cdn.discordapp.com/attachments/778630432878362676/848807810690449408/t_1.txt\n",
        "!wget https://cdn.discordapp.com/attachments/778630432878362676/848930769942478878/chars2_2.txt\n",
        "!wget https://cdn.discordapp.com/attachments/778630432878362676/848955187430424676/wordChars_2_1.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-07 16:43:38--  https://cdn.discordapp.com/attachments/778630432878362676/848807810690449408/t_1.txt\n",
            "Resolving cdn.discordapp.com (cdn.discordapp.com)... 162.159.130.233, 162.159.135.233, 162.159.129.233, ...\n",
            "Connecting to cdn.discordapp.com (cdn.discordapp.com)|162.159.130.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1028 (1.0K) [text/plain]\n",
            "Saving to: ‘t_1.txt’\n",
            "\n",
            "t_1.txt             100%[===================>]   1.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-07 16:43:39 (161 MB/s) - ‘t_1.txt’ saved [1028/1028]\n",
            "\n",
            "--2021-06-07 16:43:39--  https://cdn.discordapp.com/attachments/778630432878362676/848930769942478878/chars2_2.txt\n",
            "Resolving cdn.discordapp.com (cdn.discordapp.com)... 162.159.129.233, 162.159.130.233, 162.159.135.233, ...\n",
            "Connecting to cdn.discordapp.com (cdn.discordapp.com)|162.159.129.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95 [text/plain]\n",
            "Saving to: ‘chars2_2.txt’\n",
            "\n",
            "chars2_2.txt        100%[===================>]      95  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-07 16:43:40 (33.0 MB/s) - ‘chars2_2.txt’ saved [95/95]\n",
            "\n",
            "--2021-06-07 16:43:40--  https://cdn.discordapp.com/attachments/778630432878362676/848955187430424676/wordChars_2_1.txt\n",
            "Resolving cdn.discordapp.com (cdn.discordapp.com)... 162.159.129.233, 162.159.130.233, 162.159.135.233, ...\n",
            "Connecting to cdn.discordapp.com (cdn.discordapp.com)|162.159.129.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76 [text/plain]\n",
            "Saving to: ‘wordChars_2_1.txt’\n",
            "\n",
            "wordChars_2_1.txt   100%[===================>]      76  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-07 16:43:40 (12.4 MB/s) - ‘wordChars_2_1.txt’ saved [76/76]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6svqRPVggUw"
      },
      "source": [
        "import codecs\n",
        "import unicodedata\n",
        "from itertools import groupby\n",
        "import numpy as np\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import re\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jchl17Oc9jxN"
      },
      "source": [
        "class Node:\n",
        "    \"class representing nodes in a prefix tree\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.children = {}  # all child elements beginning with current prefix\n",
        "        self.isWord = False  # does this prefix represent a word\n",
        "\n",
        "    def __str__(self):\n",
        "        s = ''\n",
        "        for k in self.children.keys():\n",
        "            s += k\n",
        "        return 'isWord: ' + str(self.isWord) + '; children: ' + s\n",
        "\n",
        "\n",
        "class PrefixTree:\n",
        "    \"prefix tree\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.root = Node()\n",
        "\n",
        "    def addWord(self, text):\n",
        "        \"add word to prefix tree\"\n",
        "        node = self.root\n",
        "        for i in range(len(text)):\n",
        "            c = text[i]  # current char\n",
        "            if c not in node.children:\n",
        "                node.children[c] = Node()\n",
        "            node = node.children[c]\n",
        "            isLast = (i + 1 == len(text))\n",
        "            if isLast:\n",
        "                node.isWord = True\n",
        "\n",
        "    def addWords(self, words):\n",
        "        for w in words:\n",
        "            self.addWord(w)\n",
        "\n",
        "    def getNode(self, text):\n",
        "        \"get node representing given text\"\n",
        "        node = self.root\n",
        "        for c in text:\n",
        "            if c in node.children:\n",
        "                node = node.children[c]\n",
        "            else:\n",
        "                return None\n",
        "        return node\n",
        "\n",
        "    def isWord(self, text):\n",
        "        node = self.getNode(text)\n",
        "        if node:\n",
        "            return node.isWord\n",
        "        return False\n",
        "\n",
        "    def getNextChars(self, text):\n",
        "        \"get all characters which may directly follow given text\"\n",
        "        chars = []\n",
        "        node = self.getNode(text)\n",
        "        if node:\n",
        "            for k in node.children.keys():\n",
        "                chars.append(k)\n",
        "        return chars\n",
        "\n",
        "    def getNextWords(self, text):\n",
        "        \"get all words of which given text is a prefix (including the text itself, it is a word)\"\n",
        "        words = []\n",
        "        node = self.getNode(text)\n",
        "        if node:\n",
        "            nodes = [node]\n",
        "            prefixes = [text]\n",
        "            while len(nodes) > 0:\n",
        "                # put all children into list\n",
        "                for k, v in nodes[0].children.items():\n",
        "                    nodes.append(v)\n",
        "                    prefixes.append(prefixes[0] + k)\n",
        "\n",
        "                # is current node a word\n",
        "                if nodes[0].isWord:\n",
        "                    words.append(prefixes[0])\n",
        "\n",
        "                # remove current node\n",
        "                del nodes[0]\n",
        "                del prefixes[0]\n",
        "\n",
        "        return words\n",
        "\n",
        "    def dump(self):\n",
        "        nodes = [self.root]\n",
        "        while len(nodes) > 0:\n",
        "            # put all children into list\n",
        "            for v in nodes[0].children.values():\n",
        "                nodes.append(v)\n",
        "\n",
        "            # dump current node\n",
        "            print(nodes[0])\n",
        "\n",
        "            # remove from list\n",
        "            del nodes[0]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm3_H-n2R7qh"
      },

      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6i-IDL9tBW"
      },
      "source": [
        "class Optical:\n",
        "    \"optical score of beam\"\n",
        "\n",
        "    def __init__(self, prBlank=0, prNonBlank=0):\n",
        "        self.prBlank = prBlank  # prob of ending with a blank\n",
        "        self.prNonBlank = prNonBlank  # prob of ending with a non-blank\n",
        "\n",
        "\n",
        "class Textual:\n",
        "    \"textual score of beam\"\n",
        "\n",
        "    def __init__(self, text=''):\n",
        "        self.text = text\n",
        "        self.wordHist = []  # history of words so far\n",
        "        self.wordDev = ''  # developing word\n",
        "        self.prUnnormalized = 1.0\n",
        "        self.prTotal = 1.0\n",
        "\n",
        "\n",
        "class Beam:\n",
        "    \"beam with text, optical and textual score\"\n",
        "\n",
        "    def __init__(self, lm, useNGrams):\n",
        "        \"creates genesis beam\"\n",
        "        self.optical = Optical(1.0, 0.0)\n",
        "        self.textual = Textual('')\n",
        "        self.lm = lm\n",
        "        self.useNGrams = useNGrams\n",
        "\n",
        "    def mergeBeam(self, beam):\n",
        "        \"merge probabilities of two beams with same text\"\n",
        "\n",
        "        if self.getText() != beam.getText():\n",
        "            raise Exception('mergeBeam: texts differ')\n",
        "\n",
        "        self.optical.prNonBlank += beam.getPrNonBlank()\n",
        "        self.optical.prBlank += beam.getPrBlank()\n",
        "\n",
        "    def getText(self):\n",
        "        return self.textual.text\n",
        "\n",
        "    def getPrBlank(self):\n",
        "        return self.optical.prBlank\n",
        "\n",
        "    def getPrNonBlank(self):\n",
        "        return self.optical.prNonBlank\n",
        "\n",
        "    def getPrTotal(self):\n",
        "        return self.getPrBlank() + self.getPrNonBlank()\n",
        "\n",
        "    def getPrTextual(self):\n",
        "        return self.textual.prTotal\n",
        "\n",
        "    def getNextChars(self):\n",
        "        return self.lm.getNextChars(self.textual.wordDev)\n",
        "\n",
        "    def createChildBeam(self, newChar, prBlank, prNonBlank):\n",
        "        \"extend beam by new character and set optical score\"\n",
        "        beam = Beam(self.lm, self.useNGrams)\n",
        "\n",
        "        # copy textual information\n",
        "        beam.textual = copy.deepcopy(self.textual)\n",
        "        beam.textual.text += newChar\n",
        "\n",
        "        # do textual calculations only if beam gets extended\n",
        "        if newChar != '':\n",
        "            if self.useNGrams:  # use unigrams and bigrams\n",
        "\n",
        "                # if new char occurs inside a word\n",
        "                if newChar in beam.lm.getWordChars():\n",
        "                    beam.textual.wordDev += newChar\n",
        "                    nextWords = beam.lm.getNextWords(beam.textual.wordDev)\n",
        "\n",
        "                    # no complete word in text, then use unigram of all possible next words\n",
        "                    numWords = len(beam.textual.wordHist)\n",
        "                    prSum = 0\n",
        "                    if numWords == 0:\n",
        "                        for w in nextWords:\n",
        "                            prSum += beam.lm.getUnigramProb(w)\n",
        "                    # take last complete word and sum up bigrams of all possible next words\n",
        "                    else:\n",
        "                        lastWord = beam.textual.wordHist[-1]\n",
        "                        for w in nextWords:\n",
        "                            prSum += beam.lm.getBigramProb(lastWord, w)\n",
        "                    beam.textual.prTotal = beam.textual.prUnnormalized * prSum\n",
        "                    beam.textual.prTotal = beam.textual.prTotal ** (\n",
        "                            1 / (numWords + 1)) if numWords >= 1 else beam.textual.prTotal\n",
        "\n",
        "                # if new char does not occur inside a word\n",
        "                else:\n",
        "                    # if current word is not empty, add it to history\n",
        "                    if beam.textual.wordDev != '':\n",
        "                        beam.textual.wordHist.append(beam.textual.wordDev)\n",
        "                        beam.textual.wordDev = ''\n",
        "\n",
        "                        # score with unigram (first word) or bigram (all other words) probability\n",
        "                        numWords = len(beam.textual.wordHist)\n",
        "                        if numWords == 1:\n",
        "                            beam.textual.prUnnormalized *= beam.lm.getUnigramProb(beam.textual.wordHist[-1])\n",
        "                            beam.textual.prTotal = beam.textual.prUnnormalized\n",
        "                        elif numWords >= 2:\n",
        "                            beam.textual.prUnnormalized *= beam.lm.getBigramProb(beam.textual.wordHist[-2],\n",
        "                                                                                 beam.textual.wordHist[-1])\n",
        "                            beam.textual.prTotal = beam.textual.prUnnormalized ** (1 / numWords)\n",
        "\n",
        "            else:  # don't use unigrams and bigrams, just keep wordDev up to date\n",
        "                if newChar in beam.lm.getWordChars():\n",
        "                    beam.textual.wordDev += newChar\n",
        "                else:\n",
        "                    beam.textual.wordDev = ''\n",
        "\n",
        "        # set optical information\n",
        "        beam.optical.prBlank = prBlank\n",
        "        beam.optical.prNonBlank = prNonBlank\n",
        "        return beam\n",
        "\n",
        "    def __str__(self):\n",
        "        return '\"' + self.getText() + '\"' + ';' + str(self.getPrTotal()) + ';' + str(self.getPrTextual()) + ';' + str(\n",
        "            self.textual.prUnnormalized)\n",
        "\n",
        "\n",
        "class BeamList:\n",
        "    \"list of beams at specific time-step\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.beams = {}\n",
        "\n",
        "    def addBeam(self, beam):\n",
        "        \"add or merge new beam into list\"\n",
        "        # add if text not yet known\n",
        "        if beam.getText() not in self.beams:\n",
        "            self.beams[beam.getText()] = beam\n",
        "        # otherwise merge with existing beam\n",
        "        else:\n",
        "            self.beams[beam.getText()].mergeBeam(beam)\n",
        "\n",
        "    def getBestBeams(self, num):\n",
        "        \"return best beams, specify the max. number of beams to be returned (beam width)\"\n",
        "        u = [v for (_, v) in self.beams.items()]\n",
        "        lmWeight = 1\n",
        "        return sorted(u, reverse=True, key=lambda x: x.getPrTotal() * (x.getPrTextual() ** lmWeight))[:num]\n",
        "\n",
        "    def deletePartialBeams(self, lm):\n",
        "        \"delete beams for which last word is not finished\"\n",
        "        for (k, v) in self.beams.items():\n",
        "            lastWord = v.textual.wordDev\n",
        "            if (lastWord != '') and (not lm.isWord(lastWord)):\n",
        "                del self.beams[k]\n",
        "\n",
        "    def completeBeams(self, lm):\n",
        "        \"complete beams such that last word is complete word\"\n",
        "        for (_, v) in self.beams.items():\n",
        "            lastPrefix = v.textual.wordDev\n",
        "            if lastPrefix == '' or lm.isWord(lastPrefix):\n",
        "                continue\n",
        "\n",
        "            # get word candidates for this prefix\n",
        "            words = lm.getNextWords(lastPrefix)\n",
        "            # if there is just one candidate, then the last prefix can be extended to\n",
        "            if len(words) == 1:\n",
        "                word = words[0]\n",
        "                v.textual.text += word[len(lastPrefix) - len(word):]\n",
        "\n",
        "    def dump(self):\n",
        "        for k in self.beams.keys():\n",
        "            print(unicode(self.beams[k]).encode('ascii', 'replace'))  # map to ascii if possible (for py2 and windows)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXxuyFx39ayl"
      },
      "source": [
        "class LanguageModel:\n",
        "    \"unigram/bigram LM, add-k smoothing\"\n",
        "\n",
        "    def __init__(self, corpus, chars, wordChars):\n",
        "        \"read text from filename, specify chars which are contained in dataset, specify chars which form words\"\n",
        "        # read from file\n",
        "        self.wordCharPattern = '[' + wordChars + ']'\n",
        "        self.wordPattern = self.wordCharPattern + '+'\n",
        "        words = re.findall(self.wordPattern, corpus)\n",
        "        uniqueWords =  list(set(words)) \n",
        "        self.numWords = len(words)\n",
        "        self.numUniqueWords = len(uniqueWords)\n",
        "        self.smoothing = True\n",
        "        self.addK = 1.0 if self.smoothing else 0.0\n",
        "        #print(words)\n",
        "\n",
        "        # create unigrams\n",
        "        self.unigrams = {}\n",
        "        for w in words:\n",
        "            w = w.lower()\n",
        "            if w not in self.unigrams:\n",
        "                self.unigrams[w] = 0\n",
        "            self.unigrams[w] += 1 / self.numWords\n",
        "\n",
        "        # create unnormalized bigrams\n",
        "        bigrams = {}\n",
        "        for i in range(len(words) - 1):\n",
        "            w1 = words[i].lower()\n",
        "            w2 = words[i + 1].lower()\n",
        "            if w1 not in bigrams:\n",
        "                bigrams[w1] = {}\n",
        "            if w2 not in bigrams[w1]:\n",
        "                bigrams[w1][w2] = self.addK  # add-K\n",
        "            bigrams[w1][w2] += 1\n",
        "\n",
        "        # normalize bigrams\n",
        "        for w1 in bigrams.keys():\n",
        "            # sum up\n",
        "            probSum = self.numUniqueWords * self.addK  # add-K smoothing\n",
        "            for w2 in bigrams[w1].keys():\n",
        "                probSum += bigrams[w1][w2]\n",
        "            # and divide\n",
        "            for w2 in bigrams[w1].keys():\n",
        "                bigrams[w1][w2] /= probSum\n",
        "        self.bigrams = bigrams\n",
        "\n",
        "        # create prefix tree\n",
        "        self.tree = PrefixTree()  # create empty tree\n",
        "        self.tree.addWords(uniqueWords)  # add all unique words to tree\n",
        "\n",
        "        # list of all chars, word chars and nonword chars\n",
        "        self.allChars = chars\n",
        "        self.wordChars = wordChars\n",
        "        self.nonWordChars = str().join(\n",
        "            set(chars) - set(re.findall(self.wordCharPattern, chars)))  # else calculate those chars\n",
        "\n",
        "    def getNextWords(self, text):\n",
        "        \"text must be prefix of a word\"\n",
        "        return self.tree.getNextWords(text)\n",
        "\n",
        "    def getNextChars(self, text):\n",
        "        \"text must be prefix of a word\"\n",
        "        nextChars = str().join(self.tree.getNextChars(text))\n",
        "\n",
        "        # if in between two words or if word ends, add non-word chars\n",
        "        if (text == '') or (self.isWord(text)):\n",
        "            #print(text,'م',nextChars)\n",
        "            nextChars += self.getNonWordChars()\n",
        "\n",
        "        return nextChars\n",
        "\n",
        "    def getWordChars(self):\n",
        "        return self.wordChars\n",
        "\n",
        "    def getNonWordChars(self):\n",
        "        return self.nonWordChars\n",
        "\n",
        "    def getAllChars(self):\n",
        "        return self.allChars\n",
        "\n",
        "    def isWord(self, text):\n",
        "        return self.tree.isWord(text)\n",
        "\n",
        "    def getUnigramProb(self, w):\n",
        "        \"prob of seeing word w.\"\n",
        "        w = w.lower()\n",
        "        val = self.unigrams.get(w)\n",
        "        if val != None:\n",
        "            return val\n",
        "        return 0\n",
        "\n",
        "    def getBigramProb(self, w1, w2):\n",
        "        \"prob of seeing words w1 w2 next to each other.\"\n",
        "        w1 = w1.lower()\n",
        "        w2 = w2.lower()\n",
        "        val1 = self.bigrams.get(w1)\n",
        "        if val1 != None:\n",
        "            val2 = val1.get(w2)\n",
        "            if val2 != None:\n",
        "                return val2\n",
        "            return self.addK / (self.getUnigramProb(w1) * self.numUniqueWords + self.numUniqueWords)\n",
        "        return 0\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQWrqPgN9zOd"
      },
      "source": [
        "def wordBeamSearch(mat, beamWidth, lm, useNGrams):\n",
        "    \"decode matrix, use given beam width and language model\"\n",
        "    chars = lm.getAllChars()\n",
        "    blankIdx = len(chars)  # blank label is supposed to be last label in RNN output\n",
        "    maxT, _ = mat.shape  # shape of RNN output: TxC\n",
        "\n",
        "\n",
        "    genesisBeam = Beam(lm, useNGrams)  # empty string\n",
        "    last = BeamList()  # list of beams at time-step before beginning of RNN output\n",
        "    last.addBeam(genesisBeam)  # start with genesis beam\n",
        "\n",
        "    # go over all time-steps\n",
        "    for t in range(maxT):\n",
        "        curr = BeamList()  # list of beams at current time-step\n",
        "\n",
        "        # go over best beams\n",
        "        bestBeams = last.getBestBeams(beamWidth)  # get best beams\n",
        "        for beam in bestBeams:\n",
        "            # calc probability that beam ends with non-blank\n",
        "            prNonBlank = 0\n",
        "            if beam.getText() != '':\n",
        "                # char at time-step t must also occur at t-1\n",
        "                labelIdx = chars.index(beam.getText()[-1])\n",
        "                prNonBlank = beam.getPrNonBlank() * mat[t, labelIdx]\n",
        "\n",
        "            # calc probability that beam ends with blank\n",
        "            prBlank = beam.getPrTotal() * mat[t, blankIdx]\n",
        "\n",
        "            # save result\n",
        "            curr.addBeam(beam.createChildBeam('', prBlank, prNonBlank))\n",
        "\n",
        "            # extend current beam with characters according to language model\n",
        "            nextChars = beam.getNextChars()\n",
        "            for c in nextChars:\n",
        "                # extend current beam with new character\n",
        "                #print(c)\n",
        "                labelIdx = chars.index(c)\n",
        "                if beam.getText() != '' and beam.getText()[-1] == c:\n",
        "                    prNonBlank = mat[t, labelIdx] * beam.getPrBlank()  # same chars must be separated by blank\n",
        "                else:\n",
        "                    prNonBlank = mat[t, labelIdx] * beam.getPrTotal()  # different chars can be neighbours\n",
        "\n",
        "                # save result\n",
        "                curr.addBeam(beam.createChildBeam(c, 0, prNonBlank))\n",
        "\n",
        "        # move current beams to next time-step\n",
        "        last = curr\n",
        "\n",
        "    # return most probable beam\n",
        "    last.completeBeams(lm)\n",
        "    bestBeams = last.getBestBeams(1)  # sort by probability\n",
        "    return bestBeams[0].getText()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78JfeOQAqp6U"
      },
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self, chars, max_text_length=333):\n",
        "        self.chars =  list(chars)\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.maxlen = max_text_length\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = unicodedata.normalize(\"NFKD\", text).encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "        #groups = [\"\".join(group) for _, group in groupby(text)]\n",
        "        groups = [group for group in text]\n",
        "        #print(groups)\n",
        "        text = \"\".join([self.UNK_TK.join(list(x)) if len(x) > 1 else x for x in groups])\n",
        "        encoded = []\n",
        "\n",
        "        text = list(text)\n",
        "        for item in text:\n",
        "          index = self.chars.index(item)\n",
        "          index = self.UNK if index == -1 else index\n",
        "          encoded.append(index)\n",
        "        return np.asarray(encoded)\n",
        "\n",
        "    def decode(self, text):        \n",
        "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
        "        return decoded\n",
        "\n",
        "    def remove_tokens(self, text):\n",
        "        return text.replace(self.PAD_TK, \"\").replace(self.UNK_TK, \"\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAvG0e84gimA"
      },
      "source": [
        "chars = codecs.open('/content/chars2_2.txt', 'r', 'utf8').read()\n",
        "wordChars = codecs.open('/content/wordChars_2_1.txt', 'r', 'utf8').read()\n",
        "lm = LanguageModel(codecs.open('/content/t_1.txt').read(), chars, wordChars)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGwQJNXmsKDn"
      },
      "source": [
        "tokenizer = Tokenizer(chars)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkAn-kukyItt"
      },
      "source": [
        "def correct(sentence):\n",
        "  split = sentence.split()\n",
        "  sent = []\n",
        "  for token in split:\n",
        "    text = tokenizer.encode(token)\n",
        "    text_to_correct = []\n",
        "    for i in range(len(text)):\n",
        "      letter_list = [0.0 for i in range(len(tokenizer.chars)+1)]\n",
        "      letter_list[text[i]] = 1.0\n",
        "      letter_list[-1] = -9999\n",
        "      text_to_correct.append(letter_list)\n",
        "    text_to_correct = np.array(text_to_correct)\n",
        "    cor = wordBeamSearch(text_to_correct, 5, lm,True) \n",
        "    sent.append(cor)\n",
        "  return  \" \".join(sent)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNEYODSlEyU_"
      },
      "source": [
        "def wer(r, h):\n",
        "    import numpy\n",
        "    size = len(r)\n",
        "    d = numpy.zeros((len(r) + 1) * (len(h) + 1), dtype=numpy.uint8)\n",
        "    d = d.reshape((len(r) + 1, len(h) + 1))\n",
        "    for i in range(len(r) + 1):\n",
        "        for j in range(len(h) + 1):\n",
        "            if i == 0:\n",
        "                d[0][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][0] = i\n",
        "\n",
        "    for i in range(1, len(r) + 1):\n",
        "        for j in range(1, len(h) + 1):\n",
        "            if r[i - 1] == h[j - 1]:\n",
        "                d[i][j] = d[i - 1][j - 1]\n",
        "            else:\n",
        "                substitution = d[i - 1][j - 1] + 1\n",
        "                insertion = d[i][j - 1] + 1\n",
        "                deletion = d[i - 1][j] + 1\n",
        "                d[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    return d[len(r)][len(h)] , d[len(r)][len(h)]/size"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7XR5ioKtpMD"
      },
      "source": [
        "t_1 = \"الى التعليق رقم 2 اكيد ان لحكام العرب والمسلمين مسؤولية يتمثل ادناها في استدعاء السفراء في الصين للتشاور فليتهم يفعلونها ولو مرة ولنا نحن كشعوب مسؤولية كذالك تتمثل في مسانده اخواننا في الصين بمقاطعة البضائع الصينينة وليتنا نفعلها ولو ثلاتة اشهر\"\n",
        "t_2 = \"كذالك في مسانده الصين بمقاطعة البضائع الصينينة\"\n",
        "t_3 = \"نعم احرقها بطائراته التي تقصف المدن والمناطق السكنية احرقها بالبراميل التي ترمى من طائرات روسيا احرقها بمدافعه الثقيلة وذخيرته الايرانية احرق حمص لكي يبقى الطريق الى حزب الله بيده ولكي يؤمن ممر نجاة لدولته العلوية التي يسعى لها في حال سقوط مستعمرته سورية\"\n",
        "t_4= 'وهي موجهة على الاغلب ضد الشيعة و في مناطق مكتظة بالشسيعة والسنة'\n",
        "\n",
        "correct_sent = [\n",
        "           'الى التعليق رقم 2 اكيد ان للحكام العرب والمسلمين مسؤولية يتمثل ادناها في استدعاء السفراء في الصين للتشاور فليتهم يفعلونها ولو مرة ولنا نحن كشعوب مسؤولية كذلك تتمثل في مساندة اخواننا في الصين بمقاطعة البضائع الصينية وليتنا نفعلها ولو ثلاتة اشهر'\n",
        "          , 'كذلك في مساندة الصين بمقاطعة البضائع الصينية'\n",
        "           ,'نعم احرقها بطائراته التي تقصف المدن والمناطق السكنية احرقها بالبراميل التي ترمى من طائرات روسيا احرقها بمدافعه الثقيلة وذخيرته الايرانية احرق حمص لكي يبقى الطريق الى حزب الله بيده ولكي يؤمن ممر نجاة لدولته العلوية التي يسعى لها في حال سقوط مستعمرته سوريا',\n",
        "           'وهي موجهة على الاغلب ضد الشيعة و في مناطق مكتظة بالشيعة والسنة']"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SE7m1ylCXen",
        "outputId": "fd6d89af-c457-49d6-e6c0-5f6996ff133e"
      },
      "source": [
        "t_2_co = correct(t_2)\n",
        "print(' original text = ',correct_sent[1],'\\n','Wrong text = ',t_2,'\\n','corrected text = ',t_2_co)\n",
        "print(' WER = ',wer(correct_sent[1].split(),t_2_co.split())[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " original text =  كذلك في مساندة الصين بمقاطعة البضائع الصينية \n",
            " Wrong text =  كذالك في مسانده الصين بمقاطعة البضائع الصينينة \n",
            " corrected text =  كذلك في مساندة الصين بمقاطعة البضائع الصينية\n",
            " WER =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_n6LxgiqVve",
        "outputId": "f451f054-4170-42c7-957a-cffea5144ed8"
      },
      "source": [
        "t_1_co = correct(t_1)\n",
        "print(' original text = ',correct_sent[0],'\\n','Wrong text = ',t_1,'\\n','corrected text = ',t_1_co)\n",
        "print(' WER = ',wer(correct_sent[0].split(),t_1_co.split())[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " original text =  الى التعليق رقم 2 اكيد ان للحكام العرب والمسلمين مسؤولية يتمثل ادناها في استدعاء السفراء في الصين للتشاور فليتهم يفعلونها ولو مرة ولنا نحن كشعوب مسؤولية كذلك تتمثل في مساندة اخواننا في الصين بمقاطعة البضائع الصينية وليتنا نفعلها ولو ثلاتة اشهر \n",
            " Wrong text =  الى التعليق رقم 2 اكيد ان لحكام العرب والمسلمين مسؤولية يتمثل ادناها في استدعاء السفراء في الصين للتشاور فليتهم يفعلونها ولو مرة ولنا نحن كشعوب مسؤولية كذالك تتمثل في مسانده اخواننا في الصين بمقاطعة البضائع الصينينة وليتنا نفعلها ولو ثلاتة اشهر \n",
            " corrected text =  الى التعليق رقم 2 اكيد ان لها العرب والمسلمين مستعمرته يتمثل ادناها في استدعاء السفراء في الصين لها فليتهم يفعلونها ولو مرة ولنا نحن كشعوب مستعمرته كذلك تتمثل في مساندة اخواننا في الصين بمقاطعة البضائع الصينية وليتنا نفعلها ولو ثلاتة اشهر\n",
            " WER =  0.0975609756097561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCAkWNqqsPpy",
        "outputId": "9acbf356-7302-4dd1-9b17-760bfc55bafb"
      },
      "source": [
        "t_3_co = correct(t_3)\n",
        "print(' original text = ',correct_sent[2],'\\n','Wrong text = ',t_3,'\\n','corrected text = ',t_3_co)\n",
        "print(' WER = ',wer(correct_sent[2].split(),t_3_co.split())[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " original text =  نعم احرقها بطائراته التي تقصف المدن والمناطق السكنية احرقها بالبراميل التي ترمى من طائرات روسيا احرقها بمدافعه الثقيلة وذخيرته الايرانية احرق حمص لكي يبقى الطريق الى حزب الله بيده ولكي يؤمن ممر نجاة لدولته العلوية التي يسعى لها في حال سقوط مستعمرته سوريا \n",
            " Wrong text =  نعم احرقها بطائراته التي تقصف المدن والمناطق السكنية احرقها بالبراميل التي ترمى من طائرات روسيا احرقها بمدافعه الثقيلة وذخيرته الايرانية احرق حمص لكي يبقى الطريق الى حزب الله بيده ولكي يؤمن ممر نجاة لدولته العلوية التي يسعى لها في حال سقوط مستعمرته سورية \n",
            " corrected text =  نعم احرقها بطائراته التي تقصف المدن والمناطق السكنية احرقها بالبراميل التي ترمى من طائرات روسيا احرقها بمدافعه الثقيلة وذخيرته الايرانية احرقها حمص لكي يبقى الطريق الى حزب الا بيده ولكي يبقى من نجاة لدولته العلوية التي يسعى لها في حال سقوط مستعمرته سوريا\n",
            " WER =  0.09302325581395349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g3SwQYSJQqA",
        "outputId": "e98abc3c-705e-4f49-90fd-20d2406a1442"
      },
      "source": [
        "t_4_co = correct(t_4)\n",
        "print(' original text = ',correct_sent[3],'\\n','Wrong text = ',t_4,'\\n','corrected text = ',t_4_co)\n",
        "print(' WER = ',wer(correct_sent[3].split(),t_4_co.split())[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " original text =  وهي موجهة على الاغلب ضد الشيعة و في مناطق مكتظة بالشيعة والسنة \n",
            " Wrong text =  وهي موجهة على الاغلب ضد الشيعة و في مناطق مكتظة بالشسيعة والسنة \n",
            " corrected text =  وهي موجهة على الاغلب ضد الشيعة و في مناطق مكتظة بالشيعة والسنة\n",
            " WER =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbPRYg-AZZFB",
        "outputId": "3bba8552-d2b9-45b8-c37f-cf3f1fe1cb3f"
      },
      "source": [
        "total_wer()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total error rate: 0.10667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
